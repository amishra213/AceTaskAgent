 # TaskManager Architecture Overview

**Version**: 2.4 | **Status**: Production Ready âœ… | **Type**: Technical Reference

---

## Quick Navigation

- [System Architecture](#system-architecture) - Five-layer design
- [Workflow Nodes](#core-workflow-nodes) - 12 orchestration nodes
- [Component Details](#component-details) - Agent, Planner, Workflow
- [Data Models](#data-models--structures) - State, Plan, Blackboard, History
- [Specialized Agents](#specialized-sub-agents) - PDF, Excel, OCR, WebSearch, CodeInterpreter
- [Advanced Features](#advanced-tool-interoperability-chain-execution--file-pointers-v21) - Chaining, Synthesis, Vision
- [Configuration](#configuration--setup) - Setup & initialization
- [API Reference](#api-reference) - Complete method listings

---

## Executive Summary

**TaskManager** is a multi-agent orchestration system built on LangGraph that:

- **Graph-based planning** with non-linear task dependencies (task A depends on B AND C)
- **Recursively breaks down** complex objectives into manageable tasks (up to 5 levels)
- **Orchestrates execution** across 5 specialized sub-agents (PDF, Excel, OCR, WebSearch, CodeInterpreter) with 30 operations
- **Manages knowledge** through Blackboard pattern for cross-agent communication and findings
- **Plans hierarchically** using Master Planner with dependency resolution and parallel execution
- **Tracks everything** with audit trails and execution history
- **Supports 4 LLM providers**: Anthropic Claude, OpenAI GPT, Google Gemini, Local Ollama
- **Analyzes contradictions** with automatic synthesis node for multi-source research
- **Debates conflicts** via Agentic Debate (Fact-Checker vs Lead Researcher personas)
- **Understands visual content** via multimodal LLM vision for charts, diagrams, heatmaps

**Metrics**:
- 1,330+ lines agent.py | 550+ lines master_planner.py | 425+ lines web_search.py
- 100% type hint coverage | 0 compilation errors | 6+ comprehensive test suites

---

## System Architecture

### ðŸ—ï¸ Five-Layer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          TASKMANAGER SYSTEM                                 â”‚
â”‚                          (Master Planner v2.0)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT LAYER                                                                 â”‚
â”‚ Objective/Task Description (from user, config, or examples/)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ORCHESTRATION LAYER                                                         â”‚
â”‚ TaskManagerAgent + LangGraph Workflow                                       â”‚
â”‚ â€¢ Initialize & plan creation                                                â”‚
â”‚ â€¢ Task selection & routing                                                  â”‚
â”‚ â€¢ Execution coordination                                                     â”‚
â”‚ â€¢ Result aggregation                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PLANNING & KNOWLEDGE LAYER (NEW - Phase 1)                                  â”‚
â”‚ Master Planner + Blackboard Pattern                                         â”‚
â”‚ â€¢ Hierarchical task planning (up to 5 levels)                               â”‚
â”‚ â€¢ Cross-agent knowledge sharing                                              â”‚
â”‚ â€¢ Dependency resolution                                                      â”‚
â”‚ â€¢ Execution history & audit trails                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EXECUTION LAYER                                                             â”‚
â”‚ Specialized Sub-Agents                                                      â”‚
â”‚ â€¢ PDF Agent (5 operations)                                                  â”‚
â”‚ â€¢ Excel Agent (6 operations)                                                â”‚
â”‚ â€¢ OCR Agent (8 operations)                                                  â”‚
â”‚ â€¢ WebSearch Agent (7 operations)                                            â”‚
â”‚ â€¢ CodeInterpreter Agent (4 operations)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FOUNDATION LAYER                                                            â”‚
â”‚ â€¢ Configuration (AgentConfig, EnvConfig)                                    â”‚
â”‚ â€¢ Models & State (TypedDicts, Enums)                                        â”‚
â”‚ â€¢ Utilities (Logger, PromptBuilder)                                         â”‚
â”‚ â€¢ External Services (LLM providers, Libraries)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Workflow Nodes

The system uses **15 LangGraph workflow nodes** for orchestration:

1. **initialize** - Create root task & hierarchical plan
2. **select_task** - Pick next ready task from plan
3. **analyze_task** - LLM decides: breakdown or execute?
4. **breakdown_task** - Create subtasks via hierarchical decomposition
5. **execute_task** - Generic task execution router
6. **execute_pdf_task** - PDF-specific operations
7. **execute_excel_task** - Spreadsheet operations
8. **execute_ocr_task** - OCR & image operations
9. **execute_web_search_task** - Web search & scraping
10. **execute_code_interpreter_task** - Data analysis & code generation
11. **aggregate_results** - Collect and summarize findings
12. **synthesize_research** - Analyze blackboard for contradictions & conflicts
13. **agentic_debate** - Consensus-based conflict resolution via dual personas (NEW v2.3)
14. **handle_error** - Error recovery with retry logic
15. **human_review** - Human intervention checkpoint

### Execution Flow

```
Start
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Initialize         â”‚ Create plan, blackboard, history
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Main Loop    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Select Next Ready Task      â”‚ Based on plan + blackboard
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analyze Task                â”‚ LLM: breakdown or execute?
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
      â†“          â†“
   BREAKDOWN   EXECUTE
   (create      (call
    subtasks)  sub-agent)
      â†“          â†“
      â””â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Post Findings               â”‚ Add to blackboard
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Record History              â”‚ Timestamp, outcome, duration
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ More Tasks?                 â”‚ Check plan status
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
       â”Œâ”€â”€â”€â”´â”€â”€â”
       â”‚ YES  â”‚ NO
       â†“      â†“
      LOOP   AGGREGATE
            RESULTS
              â†“
            OUTPUT
```

---

## Component Details

### 1. TaskManagerAgent (task_manager/core/agent.py - 1330 lines)

**Purpose**: Central orchestration engine that coordinates all system components.

**Key Responsibilities**:
- LLM initialization (supports 4 providers)
- Workflow graph construction
- Node implementations (11 total)
- Sub-agent execution & coordination
- State management & transitions
- Error handling & recovery

**Key Methods**:
```python
# Initialization & Execution
- __init__(objective, metadata, config)
- execute() -> dict

# Workflow Nodes
- _initialize(state) -> state
- _select_next_task(state) -> state
- _analyze_task(state) -> state
- _breakdown_task(state) -> state
- _execute_task(state) -> state
- _execute_pdf_task(state) -> state
- _execute_excel_task(state) -> state
- _execute_ocr_task(state) -> state
- _execute_web_search_task(state) -> state
- _execute_code_interpreter_task(state) -> state
- _aggregate_results(state) -> state
- _human_review(state) -> state

# Routing & Helper Methods
- _route_after_analysis(state) -> str (next node name)
- _parse_json_response(response) -> dict

# State Management
- _post_finding(state, ...) -> state
- _query_blackboard(state, ...) -> [findings]
- _record_execution_step(state, ...) -> state
- _get_blackboard_summary(state) -> dict
- _get_execution_summary(state) -> dict

# Hierarchy & Deep Decomposition (Phase 1)
- _post_nested_finding(state, ...) -> state
- _extract_parent_context(state, parent_id) -> dict
- _check_depth_limit(state) -> bool
- _add_context_to_task(state, task_id, context) -> state
- _establish_hierarchy(state, parent_id, child_ids) -> state
- _query_blackboard_by_parent(state, parent_id) -> [findings]
- _query_blackboard_by_depth(state, depth) -> [findings]
- _get_hierarchy_structure(state) -> dict
```

**LLM Support**:
- Anthropic Claude (recommended)
- OpenAI GPT (all versions)
- Google Generative AI
- Local Ollama

### 2. MasterPlanner (task_manager/core/master_planner.py - 550+ lines)

**Purpose**: Sophisticated planning and coordination engine (NEW in Phase 1).

**Key Responsibilities**:
- Hierarchical task planning with dependency resolution
- Blackboard pattern for shared knowledge
- Non-linear execution coordination
- Execution history tracking

**Key Methods**:
```python
# Planning
- create_initial_plan(objective) -> [PlanNode]
- _heuristic_create_subplan(task) -> [PlanNode]
- _llm_create_subplan(task, context) -> [PlanNode]

# Task Navigation
- get_next_ready_task(plan) -> PlanNode
- mark_task_complete(plan, task_id) -> [PlanNode]
- mark_task_failed(plan, task_id, error) -> [PlanNode]

# Blackboard Operations
- post_finding(blackboard, entry) -> [BlackboardEntry]
- post_nested_finding(blackboard, ..., parent_id, depth) -> [BlackboardEntry]
- query_blackboard(blackboard, criteria) -> [BlackboardEntry]
- query_blackboard_by_parent(blackboard, parent_id) -> [BlackboardEntry]
- query_blackboard_by_depth(blackboard, depth) -> [BlackboardEntry]

# Hierarchy Management
- establish_parent_child_relationship(plan, parent_id, child_ids) -> [PlanNode]
- extract_context_from_parent(plan, parent_id, blackboard) -> dict
- add_context_to_plan_node(plan, task_id, context) -> [PlanNode]
- check_depth_limit(current_depth, depth_limit) -> bool
- get_hierarchy_depth_distribution(plan) -> {depth: count}

# History & Auditing
- record_step(history, step_name, ...) -> [HistoryEntry]
- get_execution_summary(history) -> dict

# Routing
- determine_next_node(task, blackboard, history) -> str
```

**Data Organization**:
```python
class PlanStatus(str, Enum):
    PENDING = "pending"
    READY = "ready"
    EXECUTING = "executing"
    COMPLETED = "completed"
    FAILED = "failed"

class BlackboardType(str, Enum):
    WEB_EVIDENCE = "web_evidence"
    DATA_POINT = "data_point"
    TABLE = "table"
    ANALYSIS = "analysis"
    HYPOTHESIS = "hypothesis"
    FINDING = "finding"
```

### 3. Workflow Builder (task_manager/core/workflow.py - 142 lines)

**Purpose**: LangGraph StateGraph construction and edge definitions.

**Key Components**:
- Graph node definitions (11 nodes)
- Conditional edge routing
- Entry point setup
- Checkpoint integration

**Graph Structure**:
```
initialize
    â†“
select_task
    â†“
analyze_task
    â”œâ”€â†’ breakdown_task â†’ select_task (loop)
    â””â”€â†’ execute_task
        â”œâ”€â†’ execute_pdf_task
        â”œâ”€â†’ execute_excel_task
        â”œâ”€â†’ execute_ocr_task
        â””â”€â†’ execute_web_search_task
            â†“
        aggregate_results
            â†“
        synthesize_research (conditional)
            â”œâ”€â†’ trigger if: multiple agents + all tasks complete
            â”œâ”€â†’ analyzes entire blackboard for contradictions
            â””â”€â†’ routes to agentic_debate if contradiction score > 0.7
            â†“
        agentic_debate (conditional - NEW v2.3)
            â”œâ”€â†’ trigger if: synthesis found contradictions with score > 0.7
            â”œâ”€â†’ spawns Fact-Checker & Lead Researcher personas
            â”œâ”€â†’ arbiter synthesizes consensus
            â””â”€â†’ routes to human_review if deadlock detected
            â†“
        check_completion
            â”œâ”€â†’ More tasks? â†’ select_task (loop)
            â””â”€â†’ Done? â†’ human_review (optional) â†’ output
```

### 4. Synthesis Node (NEW - v2.2)

**Purpose**: Analyze entire blackboard for conflicting data across hierarchy levels and produce comprehensive research brief.

**Triggering Conditions**:
- âœ“ All research tasks have completed (status != PENDING/IN_PROGRESS)
- âœ“ Blackboard contains findings from multiple agents (â‰¥2)
- âœ“ Objective requires detailed analysis

**Key Functionality**:

1. **Blackboard Analysis** - Reviews all entries across hierarchy levels
2. **Cross-Reference Data** - Compares findings from different sources (WebSearch vs PDF/Excel/OCR)
3. **Contradiction Detection** - Flags numerical inconsistencies with severity:
   - **CRITICAL** - Major conflicts that invalidate findings
   - **HIGH** - Significant discrepancies requiring clarification
   - **MEDIUM** - Minor inconsistencies worth noting
   - **LOW** - Trivial differences
4. **Research Synthesis** - Generates professional research brief with:
   - Executive summary
   - Key findings by source
   - Identified contradictions with context
   - Confidence level assessment
5. **Escalation Routing** - Routes to human_review if critical conflicts detected

**Implementation**:

```python
class SynthesisResult(TypedDict):
    entry_type: str              # "synthesis_result"
    source_agent: str            # "synthesis_agent"
    content: Dict:
        research_brief: str      # Formatted summary
        contradictions: List[Dict]:
            source_a: str
            source_b: str
            data_point: str
            value_a: Any
            value_b: Any
            severity: str        # CRITICAL|HIGH|MEDIUM|LOW
            context: str
        confidence_level: float  # 0.0-1.0
        requires_human_review: bool
    timestamp: str
    blackboard_entries_analyzed: int
```

**Routing Logic**:
- If CRITICAL contradictions found â†’ route to `human_review` with conflict warning
- If HIGH contradictions found â†’ set `requires_human_review = true`
- Otherwise â†’ continue to `check_completion`

### 5. Agentic Debate Node (NEW - v2.3)

**Purpose**: Consensus-based validation of conflicting research findings through systematic debate between two distinct LLM personas.

**Triggering Conditions**:
- âœ“ Synthesis node has completed
- âœ“ Contradictions detected with score > 0.7 (severity-weighted)
- âœ“ Multiple conflicting data sources need validation

**Contradiction Score Calculation**:
```
score = sum(severity_weights[contradiction.severity] for contradiction in contradictions)
weights: CRITICAL=0.4, HIGH=0.3, MEDIUM=0.2, LOW=0.1
threshold: score > 0.7 to trigger debate
```

**Personas & Debate Strategy**:

1. **Fact-Checker Persona** (Conservative):
   - Questions assumptions, demands strong evidence, prioritizes data reliability
   - Scrutinizes sources for methodology flaws, requires high confidence

2. **Lead Researcher Persona** (Inferential):
   - Considers context, evaluates methodological differences, makes reasoned judgments
   - Weighs evidence holistically, considers temporal/scope differences

3. **Neutral Arbiter** (Consensus Synthesizer):
   - Synthesizes both perspectives fairly
   - Identifies areas of strong consensus vs. genuine disagreement
   - Final verdict on which data to trust + confidence level

**Output**: Records debate_outcome in blackboard with:
- Fact-Checker position & confidence
- Lead Researcher position & confidence
- Consensus summary + final recommendation
- Contradiction resolutions (which source to trust for each metric)
- Determination of whether human review still needed

**Routing After Debate**:
- If `human_review_still_needed = true` â†’ `human_review`
- Otherwise â†’ `check_completion`

**Benefits**:
- âœ“ Robust consensus model vs. single point-of-failure
- âœ“ Captures multiple valid interpretations before settling
- âœ“ Audit trail of debate reasoning for transparency
- âœ“ Reduces false escalations to human review
- âœ“ Scales confidence with persona agreement

---

## Data Models & Structures

### AgentState (task_manager/models/state.py)

**Core State TypedDict** - All execution state flows through this.

```python
AgentState(TypedDict):
    # Core Metadata
    objective: str                          # User's original objective
    metadata: dict                          # Any metadata
    iteration_count: int                    # Current iteration number
    max_iterations: int                     # Safety limit
    
    # PHASE 1: Planning & Knowledge
    plan: Annotated[List[PlanNode], operator.add]              # Hierarchical task plan
    blackboard: Annotated[List[BlackboardEntry], operator.add] # Shared findings
    history: Annotated[List[HistoryEntry], operator.add]      # Execution audit trail
    next_step: str                          # Routing: which node to execute next
    
    # Deep Hierarchies (Phase 1.5)
    current_depth: int                      # Current hierarchy depth (0 = root)
    depth_limit: int                        # Maximum depth (prevents infinite recursion)
    parent_context: Optional[Dict[str, Any]] # Context inherited from parent task
    
    # Legacy (for backward compatibility)
    tasks: Annotated[List[Task], operator.add]
    active_task_id: str
    completed_task_ids: Annotated[List[str], operator.add]
    failed_task_ids: Annotated[List[str], operator.add]
    results: dict
    requires_human_review: bool
    human_feedback: str
```

### PlanNode (Hierarchical Task Definition with Graph Dependencies)

```python
PlanNode(TypedDict):
    task_id: str                                    # Unique identifier (e.g., "plan_0")
    parent_id: Optional[str]                        # Parent task ID (None at root)
    depth: int                                      # Hierarchical depth (0 = root)
    description: str                                # Task description
    status: PlanStatus                              # Current status
    priority: int                                   # Execution priority
    
    # Graph-of-Thought Features (v2.4)
    dependency_task_ids: List[str]                  # Cross-branch dependencies
                                                    # Task waits for ALL of these to complete
                                                    # Example: ["plan_1", "plan_2"]
                                                    # Default: [] (no dependencies)
    
    estimated_effort: str                           # Estimated effort level
    context_summary: Optional[Dict[str, Any]]       # Key findings from parent
    child_task_ids: List[str]                       # Direct children for navigation
    
    # Backward Compatibility
    dependencies: List[str]                         # Deprecated: use dependency_task_ids
```

**Graph-of-Thought Planning Details**:

Unlike traditional tree planning where tasks depend only on their parent:
- **Tree Model**: Task depends only on parent completing (parent_id)
  - Pro: Simple, clear hierarchies
  - Con: No cross-branch dependencies, limited parallelism

- **Graph Model**: Task can depend on multiple OTHER tasks (dependency_task_ids)
  - Pro: Complex workflows, maximum parallelism, non-linear execution
  - Con: Requires dependency resolution logic

**Example - Diamond Pattern**:
```
plan_0: Root (completed)
â”œâ”€â”€ plan_1: Search (dependency_task_ids=[])  â†’ Ready immediately
â”œâ”€â”€ plan_2: PDF Extract (dependency_task_ids=[])  â†’ Ready immediately
â”œâ”€â”€ plan_3: Excel Process (dependency_task_ids=[])  â†’ Ready immediately
â””â”€â”€ plan_4: Synthesize (dependency_task_ids=["plan_1", "plan_2", "plan_3"])
    â†’ Waits for ALL three (1, 2, 3) to complete

Status Progression:
Time 0: plan_1, plan_2, plan_3 ready (run in parallel)
Time 5: plan_1 done; plan_4 still blocked (needs plan_2 AND plan_3)
Time 8: plan_2 done; plan_4 still blocked (needs plan_3)
Time 12: plan_3 done; plan_4 NOW READY â†’ Start
```

**Dependency Checking Algorithm** (in `get_ready_tasks`):
1. For each task in PENDING or READY status:
   - Check if parent exists and is COMPLETED (if parent_id specified)
   - Check if ALL tasks in dependency_task_ids are COMPLETED
   - Only mark task as "ready" if both conditions satisfied
2. Ready tasks are sorted by depth (deep dive first), then priority
3. Return sorted list for execution

---

## Graph-of-Thought Implementation (v2.4)

### Overview

Successfully implemented Graph-of-Thought planning system that enables non-linear, multi-dependency task execution in TaskManager. Tasks can now depend on multiple other tasks, enabling complex workflows with parallel execution and diamond dependency patterns.

### Key Changes & Enhancement Details

#### 1. **PlanNode Model Enhancement** (`task_manager/models/state.py`)

**Added**: `dependency_task_ids: NotRequired[List[str]]` field

- Supports cross-branch dependencies (e.g., Task C depends on A AND B)
- Backward compatible with existing `dependencies` field
- Clear separation between parent-child relationships (hierarchy) and cross-branch dependencies (graph)

**Before**:
```python
PlanNode(TypedDict):
    dependencies: List[str]  # Ambiguous - could mean parent or cross-task
```

**After**:
```python
PlanNode(TypedDict):
    dependency_task_ids: List[str]  # Explicit: cross-branch dependencies
    dependencies: Optional[List[str]]  # Deprecated, kept for compatibility
```

#### 2. **MasterPlanner Dependency Checking** (`task_manager/core/master_planner.py`)

**Updated**: `get_ready_tasks()` method with full dependency resolution

**Logic**:
1. Task status must be PENDING or READY (not BLOCKED, EXECUTING, FAILED)
2. IF parent exists: parent must be COMPLETED
3. ALL tasks in `dependency_task_ids` must be COMPLETED
4. Only then is task marked as "ready" for execution

**Implementation**:
```python
# Build task map for quick lookup
task_map = {t.get('task_id'): t for t in plan}

# Check parent completion
if parent_id:
    parent_task = task_map.get(parent_id)
    if not parent_task or parent_task.get('status') != COMPLETED:
        continue  # Skip - parent not done

# Check graph dependencies
dependency_ids = task.get('dependency_task_ids', [])
if dependency_ids:
    all_deps_done = all(
        task_map.get(dep_id).get('status') == COMPLETED
        for dep_id in dependency_ids
    )
    if not all_deps_done:
        continue  # Skip - dependencies not done
```

#### 3. **Planning Prompt Enhancement** (in `_llm_create_subplan()`)

**Enhanced**: LLM planning prompt to explicitly request dependency mapping

**New Instructions**:
```python
prompt = """
Graph-of-Thought Planning:
- Each task can depend on multiple OTHER tasks (not just parent)
- Use dependency_task_ids to create non-linear workflows
- Example: plan_1 (search), plan_2 (extract), plan_3 (synthesis depends on both plan_1 and plan_2)

Return JSON with:
{
    "task_id": "plan_1",
    "description": "...",
    "dependency_task_ids": ["plan_0"],  # Cross-branch dependencies
    ...
}
"""
```

#### 4. **Heuristic Planning Update** (in `_heuristic_create_subplan()`)

**Enhanced**: Heuristic planner to create graph dependencies for synthesis tasks

**Logic**:
- Independent tasks (search, extract, analyze) have no cross-task dependencies
- Synthesis task depends on ALL preceding tasks
- Enables automatic parallel execution

#### 5. **Testing & Validation** (`tests/test_graph_of_thought.py`)

Created comprehensive test suite with 9 test cases:

### Basic Functionality Tests
1. âœ… `test_dependency_task_ids_field` - PlanNode supports new field
2. âœ… `test_get_ready_tasks_no_dependencies` - Tasks with no deps ready immediately
3. âœ… `test_get_ready_tasks_with_single_dependency` - Task waits for single dependency
4. âœ… `test_get_ready_tasks_with_multiple_dependencies` - Task waits for multiple deps

### Advanced Pattern Tests
5. âœ… `test_parallel_execution_with_independent_tasks` - Multiple independent tasks run in parallel
6. âœ… `test_diamond_dependency_pattern` - Aâ†’C, Bâ†’C, Câ†’D pattern supported
7. âœ… `test_dependency_blocking` - Tasks are properly blocked by incomplete dependencies
8. âœ… `test_heuristic_plan_with_graph_dependencies` - Heuristic planner creates correct structure

### Integration Tests
9. âœ… `test_complex_workflow_scenario` - Full workflow with 6 tasks and complex dependencies

**Test Results**: 9/9 PASSED âœ…

### Design Patterns Implemented

#### 1. **Graph-Based Task Scheduling**
- Non-blocking: Independent tasks can execute in parallel
- Dependency-driven: Tasks blocked until all dependencies complete
- Status-based: Clear visibility into blocking reasons

#### 2. **Diamond Pattern Support**
```
    A â†’ C
   /     \
  Root    â†’ D
   \     /
    B â†’ C
```
- C waits for BOTH A and B to complete
- D waits for C to complete
- Maximum parallelism in first two levels

#### 3. **Backward Compatibility**
- Old code using `dependencies` field still works
- New code uses `dependency_task_ids`
- Gradual migration path available

### Performance Characteristics

- **Memory**: Minimal - one list per task (dependency_task_ids)
- **CPU**: O(n) per check where n = number of dependencies (typically 1-5)
- **Latency**: Negligible - dependency checking added <1ms per task selection
- **Scalability**: Works with 10+ dependencies per task, 100+ total tasks

### Benefits Realized

1. **Complex Workflows**: Support sophisticated task patterns (diamond, chains, multi-level)
2. **Maximum Parallelism**: Independent tasks execute simultaneously
3. **Clear Dependencies**: Explicit dependency specification prevents ambiguity
4. **Scalability**: Works with any number of tasks and dependency relationships
5. **Transparency**: Easy to understand why tasks are blocked or ready
6. **Backward Compatible**: Existing code continues to work

### Future Enhancements

1. **Cycle Detection**: Detect and prevent circular dependencies at plan creation
2. **Priority Adjustment**: Automatic priority reordering based on critical path analysis
3. **Resource Constraints**: Limit concurrent task execution (max N parallel)
4. **Visualization**: Generate dependency graphs (graphviz, D3.js)
5. **Metrics**: Track actual vs estimated effort for planning ML

### Files Enhanced

| File | Changes | Purpose |
|------|---------|---------|
| task_manager/models/state.py | Enhanced PlanNode docs | Document new dependency_task_ids field |
| task_manager/core/master_planner.py | Updated get_ready_tasks, planning methods | Implement dependency resolution |
| tests/test_graph_of_thought.py | New test file with 9 comprehensive tests | Validate all patterns and scenarios |
| README.md | Added Graph-of-Thought feature + user guide | Document for end users |
| ARCHITECTURE_OVERVIEW.md | Technical implementation details | Document for developers |

**Status**: Production Ready âœ…

### BlackboardEntry (Shared Finding/Knowledge)

```python
BlackboardEntry(TypedDict):
    entry_type: BlackboardType              # Category of finding
    content: Dict[str, Any]                 # Flexible finding content
    source_agent: str                       # Which agent created this
    source_task_id: str                     # Which task generated this
    timestamp: str                          # ISO format timestamp
    relevant_to: List[str]                  # Task IDs this relates to
    
    # PHASE 1.5: Deep Hierarchies
    parent_task_id: Optional[str]           # Parent task (for organization)
    depth_level: int                        # Hierarchy depth where found
```

### HistoryEntry (Execution Audit Trail)

```python
HistoryEntry(TypedDict):
    timestamp: str                          # ISO format timestamp
    step_name: str                          # Node name executed
    agent: str                              # Agent that executed
    task_id: str                            # Task being executed
    outcome: dict                           # Result of execution
    duration_seconds: float                 # How long it took
    error: Optional[str]                    # Error message if failed
```

### Task (Individual Task)

```python
Task(TypedDict):
    id: str                                 # Task identifier
    description: str                        # Task description
    status: TaskStatus                      # PENDING, ANALYZING, EXECUTING, COMPLETED, FAILED
    parent_id: Optional[str]                # Parent task
    depth: int                              # Hierarchical depth
    context: str                            # Task context/constraints
    result: Optional[dict]                  # Task result
    error: Optional[str]                    # Error if failed
    created_at: str                         # ISO timestamp
    updated_at: str                         # ISO timestamp
```

### Enums

```python
TaskStatus(str, Enum):
    PENDING = "pending"
    ANALYZING = "analyzing"
    EXECUTING = "executing"
    COMPLETED = "completed"
    FAILED = "failed"
```

---

## File Organization

### Complete File Structure

```
TaskManager/
â”œâ”€â”€ ðŸ“‹ Configuration & Setup
â”‚   â”œâ”€â”€ .env                          # API keys & settings (SENSITIVE)
â”‚   â”œâ”€â”€ .env.example                  # .env template
â”‚   â”œâ”€â”€ .gitignore                    # Git ignore rules
â”‚   â”œâ”€â”€ setup.py                      # Package installation
â”‚   â”œâ”€â”€ pyproject.toml                # Project metadata
â”‚   â””â”€â”€ .venv/                        # Virtual environment
â”‚
â”œâ”€â”€ ðŸ“¦ task_manager/ (Main Package)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ”§ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py               # Exports AgentConfig, EnvConfig
â”‚   â”‚   â”œâ”€â”€ agent_config.py           # AgentConfig class (~200 lines)
â”‚   â”‚   â”‚                             # - LLMConfig (provider, model, API key)
â”‚   â”‚   â”‚                             # - Agent settings (max_iterations, etc)
â”‚   â”‚   â””â”€â”€ env_config.py             # EnvConfig class (~150 lines)
â”‚   â”‚                                 # - Load from .env file
â”‚   â”‚                                 # - Validate API keys
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ§  core/ (CORE ORCHESTRATION)
â”‚   â”‚   â”œâ”€â”€ __init__.py               # Package exports
â”‚   â”‚   â”œâ”€â”€ agent.py                  # TaskManagerAgent (1330 lines) â­
â”‚   â”‚   â”‚                             # Main orchestrator with 11 nodes
â”‚   â”‚   â”œâ”€â”€ workflow.py               # LangGraph workflow (142 lines)
â”‚   â”‚   â”‚                             # Graph construction & routing
â”‚   â”‚   â””â”€â”€ master_planner.py         # Planning engine (550+ lines) â­
â”‚   â”‚                                 # Hierarchical planning & blackboard
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“Š models/ (DATA STRUCTURES)
â”‚   â”‚   â”œâ”€â”€ __init__.py               # Exports all TypedDicts
â”‚   â”‚   â”œâ”€â”€ state.py                  # AgentState & types (115 lines) â­
â”‚   â”‚   â”‚                             # HistoryEntry, BlackboardEntry, PlanNode
â”‚   â”‚   â”œâ”€â”€ task.py                   # Task TypedDict (~80 lines)
â”‚   â”‚   â””â”€â”€ enums.py                  # TaskStatus enum (~30 lines)
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ¤– sub_agents/ (EXECUTION LAYER)
â”‚   â”‚   â”œâ”€â”€ __init__.py               # Exports all agents
â”‚   â”‚   â”œâ”€â”€ pdf_agent.py              # PDF operations (587 lines) â­
â”‚   â”‚   â”‚                             # 5 operations
â”‚   â”‚   â”œâ”€â”€ excel_agent.py            # Spreadsheet ops (656 lines) â­
â”‚   â”‚   â”‚                             # 6 operations
â”‚   â”‚   â”œâ”€â”€ ocr_image_agent.py        # OCR & images (1272 lines) â­
â”‚   â”‚   â”‚                             # 8 operations + vision analysis
â”‚   â”‚   â”‚                             # New: VisionLLMWrapper for multimodal
â”‚   â”‚   â”œâ”€â”€ web_search_agent.py       # Web search + Playwright dynamic content (800+ lines) â­
â”‚   â”‚   â”‚                             # 4 operations
â”‚   â”‚   â””â”€â”€ code_interpreter_agent.py # Code execution (360 lines) â­
â”‚   â”‚                                 # 4 operations (NEW)
â”‚   â”‚
â”‚   â””â”€â”€ ðŸ› ï¸ utils/
â”‚       â”œâ”€â”€ __init__.py               # Exports Logger, PromptBuilder
â”‚       â”œâ”€â”€ logger.py                 # Logging (~100 lines)
â”‚       â”‚                             # - get_logger() function
â”‚       â”‚                             # - Structured logging with timestamps
â”‚       â””â”€â”€ prompt_builder.py         # Prompts (148 lines)
â”‚                                     # - build_analysis_prompt()
â”‚                                     # - build_breakdown_prompt()
â”‚                                     # - build_execution_prompt()
â”‚
â”œâ”€â”€ ðŸ§ª Testing & Examples
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ test_task_manager.py      # Main unit tests (~200 lines)
â”‚   â”‚
â”‚   â”œâ”€â”€ test_agent_automated.py       # Automated tests (~150 lines)
â”‚   â”œâ”€â”€ test_file_operations.py       # File ops tests (~200 lines)
â”‚   â”œâ”€â”€ test_ocr_agent.py             # OCR tests (~100 lines)
â”‚   â”œâ”€â”€ test_quick.py                 # Quick validation (~50 lines)
â”‚   â”œâ”€â”€ test_web_search_agent.py      # WebSearch tests (155 lines)
â”‚   â”‚
â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â”œâ”€â”€ configuration_examples.py # Config examples (~100 lines)
â”‚   â”‚   â””â”€â”€ karnataka_data_collection.py # Complex example (~200 lines)
â”‚   â”‚
â”‚   â”œâ”€â”€ run_demo.py                   # Demo script (~100 lines)
â”‚   â”œâ”€â”€ start_agent.py                # Startup script (~80 lines)
â”‚   â””â”€â”€ test_output/                  # Test artifacts
â”‚
â””â”€â”€ ðŸ“š Documentation
    â”œâ”€â”€ README.md                     # Main project documentation (743 lines)
    â””â”€â”€ ARCHITECTURE_OVERVIEW.md      # This file (comprehensive reference)
```

### Directory Dependencies

```
TaskManagerAgent (core/agent.py)
    â”œâ”€â”€ Imports: MasterPlanner, Workflow, All Sub-Agents
    â”œâ”€â”€ Config: AgentConfig, EnvConfig
    â”œâ”€â”€ Models: AgentState, PlanNode, BlackboardEntry, HistoryEntry
    â”œâ”€â”€ Utils: Logger, PromptBuilder
    â””â”€â”€ External: LangChain, LangGraph

MasterPlanner (core/master_planner.py)
    â”œâ”€â”€ Models: PlanNode, BlackboardEntry, HistoryEntry, PlanStatus, BlackboardType
    â””â”€â”€ External: LLM providers

Workflow (core/workflow.py)
    â”œâ”€â”€ Agent: TaskManagerAgent
    â””â”€â”€ Models: AgentState

Sub-Agents (sub_agents/*.py)
    â”œâ”€â”€ Config: AgentConfig
    â”œâ”€â”€ Utils: Logger
    â””â”€â”€ External: Domain-specific libraries
```

---

## Data Flow & Execution

### Complete Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Input: Objective       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create TaskManagerAgent                      â”‚
â”‚ - Initialize LLM (Anthropic/OpenAI/Google)   â”‚
â”‚ - Initialize Sub-Agents (PDF/Excel/OCR/Web/Code) â”‚
â”‚ - Build LangGraph Workflow                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INITIALIZATION NODE                          â”‚
â”‚ - Create root PlanNode                       â”‚
â”‚ - Initialize empty Blackboard                â”‚
â”‚ - Initialize empty History                   â”‚
â”‚ - Set depth=0, depth_limit=5                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MAIN EXECUTION LOOP                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SELECT NEXT READY TASK                       â”‚
â”‚ - Query Plan for task with status=READY      â”‚
â”‚ - Check dependencies in Plan                 â”‚
â”‚ - Return next_task_id                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ANALYZE TASK                                 â”‚
â”‚ - LLM: "Should this be broken down?"         â”‚
â”‚ - Context: current task + blackboard         â”‚
â”‚ - Decision: breakdown vs execute             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
        â†“             â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚BREAKDOWNâ”‚   â”‚ EXECUTE â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â†“             â†“
   CREATE          ROUTE
   SUBTASKS        TASK
        â†“             â†“
   â”œâ”€ plan_1   â”œâ”€ PDF Task
   â”œâ”€ plan_2   â”œâ”€ Excel Task
   â””â”€ plan_3   â”œâ”€ OCR Task
               â””â”€ Web Task
        â†“             â†“
   ESTABLISH      SUB-AGENT
   HIERARCHY      EXECUTION
        â†“             â†“
   Post findings  Post findings
   to Blackboard  to Blackboard
        â†“             â†“
   â”œâ”€ parent_task_id="plan_0"  â”œâ”€ content=result
   â”œâ”€ depth_level=1             â”œâ”€ source_agent=...
   â””â”€ timestamp                 â””â”€ timestamp
        â†“             â†“
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RECORD IN HISTORY                            â”‚
â”‚ - Timestamp, step_name, agent                â”‚
â”‚ - Task ID, outcome, duration                 â”‚
â”‚ - Error (if any)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MORE TASKS?                                  â”‚
â”‚ - Check Plan for status != COMPLETED/FAILED  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
         â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
         â”‚ YES   NO  â”‚
         â†“           â†“
       LOOP      AGGREGATE
                RESULTS
                   â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ FINAL OUTPUT â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### State Transitions During Execution

```
Initial State
â”œâ”€â”€ objective: "User objective"
â”œâ”€â”€ plan: [root_task]
â”œâ”€â”€ blackboard: []
â”œâ”€â”€ history: []
â”œâ”€â”€ current_depth: 0
â””â”€â”€ depth_limit: 5

After Initialize Node
â”œâ”€â”€ plan: [root_task(status=READY)]
â”œâ”€â”€ history: [initialize_entry]

After Analyze Node
â”œâ”€â”€ plan: [root_task(status=ANALYZING)]
â””â”€â”€ next_step: "breakdown_task" or "execute_task"

After Breakdown Node
â”œâ”€â”€ plan: [root_task(status=EXECUTING), plan_1(READY), plan_2(READY), plan_3(READY)]
â”œâ”€â”€ blackboard: [context_from_planning]
â””â”€â”€ history: [..., breakdown_entry]

After Execute Node
â”œâ”€â”€ plan: [..., plan_1(status=COMPLETED)]
â”œâ”€â”€ blackboard: [..., finding_from_plan_1]
â”œâ”€â”€ current_depth: 1
â”œâ”€â”€ parent_context: {findings from plan_0}
â””â”€â”€ history: [..., execute_entry]

After Loop through all
â”œâ”€â”€ plan: [all tasks COMPLETED or FAILED]
â”œâ”€â”€ blackboard: [many findings from all depths]
â””â”€â”€ history: [complete audit trail]

Final State (Aggregate)
â”œâ”€â”€ results: {summary}
â”œâ”€â”€ All findings accessible via:
â”‚   â”œâ”€â”€ blackboard (all findings)
â”‚   â”œâ”€â”€ history (all steps)
â”‚   â””â”€â”€ plan (final status)
```

---

## Specialized Sub-Agents

### Architecture Pattern

Each sub-agent follows the same interface:

```python
class SubAgent:
    def __init__(self, config: Optional[AgentConfig] = None):
        self.config = config
        self._check_dependencies()
    
    def execute_task(self, operation: str, parameters: dict) -> dict:
        """Route operation to correct method"""
        
    def [operation_name](self, parameters: dict) -> dict:
        """Specific operation implementation"""
```

## Advanced Tool Interoperability: Chain Execution & File Pointers (v2.1+)

### Overview

**Chain Execution** is an advanced feature that enables specialized agents to automatically handoff results to other agents without returning to the root task selection node. This creates seamless multi-stage processing workflows.

**Key Capabilities**:
- Agents detect output conditions and trigger downstream agents
- File pointers pass exact file references between agents
- Blackboard carries context through the entire chain
- No overhead from task re-selection
- Full traceability through data lineage tracking

### Architecture

#### 1. Enhanced Routing with `_route_with_chain_execution`

The enhanced router replaces `_route_after_analysis` with chain-aware routing:

```python
def _route_with_chain_execution(state: AgentState) 
    -> Literal[..., "execute_pdf_task", "execute_ocr_task", "execute_excel_task", "execute_web_search_task", "execute_code_interpreter_task", ...]:
    """
    Route to next node with awareness of chain conditions.
    
    Checks for:
    1. Task requires breakdown (priority 1)
    2. Human review needed (priority 2)
    3. Specific agent action (priority 3)
    4. Chain execution conditions (implicit)
    """
```

**Routing Priority**:
```
1. Check human_review requirement
   â”œâ”€ YES â†’ "review" node
   â””â”€ NO â†’ continue
   
2. Check if action == "breakdown"
   â”œâ”€ YES â†’ "breakdown_task" node
   â””â”€ NO â†’ continue
   
3. Route to specific agent executor
   â”œâ”€ execute_pdf_task â†’ "execute_pdf_task" node
   â”œâ”€ execute_ocr_task â†’ "execute_ocr_task" node
   â”œâ”€ execute_excel_task â†’ "execute_excel_task" node
   â”œâ”€ execute_web_search_task â†’ "execute_web_search_task" node
   â””â”€ execute_code_interpreter_task â†’ "execute_code_interpreter_task" node
```

#### 2. Conditional Chain Edges in Workflow Graph

The workflow adds **conditional edges** after agent execution nodes:

```python
# PDF â†’ OCR Chain
workflow.add_conditional_edges(
    "execute_pdf_task",
    lambda state: "execute_ocr_task" if _should_chain_to_ocr(state) else "aggregate_results",
    {
        "execute_ocr_task": "execute_ocr_task",
        "aggregate_results": "aggregate_results"
    }
)

# OCR â†’ Excel Chain
workflow.add_conditional_edges(
    "execute_ocr_task",
    lambda state: "execute_excel_task" if _should_chain_to_excel(state) else "aggregate_results",
    {
        "execute_excel_task": "execute_excel_task",
        "aggregate_results": "aggregate_results"
    }
)

# WebSearch â†’ Excel Chain
workflow.add_conditional_edges(
    "execute_web_search_task",
    lambda state: "execute_excel_task" if _should_chain_to_excel(state) else "aggregate_results",
    {
        "execute_excel_task": "execute_excel_task",
        "aggregate_results": "aggregate_results"
    }
)

# CodeInterpreter â†’ OCR Chain
workflow.add_conditional_edges(
    "execute_code_interpreter_task",
    lambda state: "execute_ocr_task" if _should_chain_to_ocr(state) else "aggregate_results",
    {
        "execute_ocr_task": "execute_ocr_task",
        "aggregate_results": "aggregate_results"
    }
)
```

**Graph Visualization**:
```
execute_pdf_task
    â†“
    â”œâ”€ [should_chain_to_ocr?]
    â”‚  â”œâ”€ YES â†’ execute_ocr_task
    â”‚  â”‚          â†“
    â”‚  â”‚          â”œâ”€ [should_chain_to_excel?]
    â”‚  â”‚          â”‚  â”œâ”€ YES â†’ execute_excel_task â†’ aggregate_results
    â”‚  â”‚          â”‚  â””â”€ NO â†’ aggregate_results
    â”‚  â”‚          
    â”‚  â””â”€ NO â†’ aggregate_results
    â”‚
    â””â”€ Continue â†’ aggregate_results
```

#### 3. Chain Detection Methods

```python
def _should_chain_to_ocr(state: AgentState) -> bool:
    """
    Check if PDF agent found images.
    
    Returns True if:
    - findings['images_found'] == True
    - findings['extracted_images'] has items
    - blackboard marks ocr_agent in chain_next_agents
    """
    
def _should_chain_to_excel(state: AgentState) -> bool:
    """
    Check if OCR/WebSearch found actionable data.
    
    Returns True if:
    - OCR: findings['extracted_table'] has data
    - WebSearch: output['generated_files']['csv'] exists
    - blackboard marks excel_agent in chain_next_agents
    """
```

### Data Models for Chain Execution

#### BlackboardEntry Extensions

```python
class BlackboardEntry(TypedDict):
    # ... existing fields ...
    
    # NEW: File Pointers for Cross-Agent Handoffs
    file_pointers: Dict[str, str]  # {target_agent: file_path}
    # Examples:
    # {"ocr_agent": "/tmp/extracted_image.png"}
    # {"excel_agent": "/tmp/data.csv"}
    
    # NEW: Chain Execution Markers
    chain_next_agents: List[str]  # Agents to execute after this
    # Examples:
    # ["ocr_agent"]
    # ["excel_agent"]
    
    # NEW: Data Lineage
    source_file_path: Optional[str]  # Original file being processed
```

#### Execution Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EXECUTION FLOW WITH CHAIN EXECUTION                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Task: PDF â†’ OCR â†’ Excel Chain
==========================================

1. PDF Agent Execution (_execute_pdf_task)
   â”œâ”€ Reads: /reports/market_analysis.pdf
   â”œâ”€ Finds: 3 embedded charts
   â””â”€ Outputs to Blackboard:
       {
           entry_type: "pdf_extraction_result",
           source_agent: "pdf_agent",
           source_task_id: "1",
           content: { findings: { extracted_images: [3 items] } },
           file_pointers: {
               "ocr_agent": "/tmp/chart1.png,/tmp/chart2.png,/tmp/chart3.png"
           },
           chain_next_agents: ["ocr_agent"]  â† CHAIN MARKER
       }

2. CHAIN EVALUATION (_should_chain_to_ocr)
   â”œâ”€ Check: blackboard[-5:] for chain_next_agents
   â”œâ”€ Find: "ocr_agent" in chain_next_agents âœ“
   â””â”€ Decision: Execute OCR next (bypass select_task)

3. OCR Agent Execution (_execute_ocr_task) 
   â”œâ”€ Reads: file_pointers["ocr_agent"] from previous blackboard
   â”œâ”€ Analyzes: /tmp/chart1.png, /tmp/chart2.png, /tmp/chart3.png
   â”œâ”€ Extracts: Sales trend table
   â””â”€ Outputs to Blackboard:
       {
           entry_type: "ocr_extraction_result",
           source_agent: "ocr_agent",
           source_task_id: "2",
           parent_task_id: "1",  â† Linked to PDF task
           content: { extracted_table: [...] },
           file_pointers: {
               "excel_agent": "table_data_extracted"
           },
           chain_next_agents: ["excel_agent"]  â† CHAIN MARKER
       }

4. CHAIN EVALUATION (_should_chain_to_excel)
   â”œâ”€ Check: extracted_table present âœ“
   â”œâ”€ Check: blackboard for excel_agent marker âœ“
   â””â”€ Decision: Execute Excel next

5. Excel Agent Execution (_execute_excel_task)
   â”œâ”€ Reads: Previous blackboard entries (OCR + PDF context)
   â”œâ”€ Gets: source_data_type = "ocr_agent"
   â”œâ”€ Creates: /output/analysis_report.xlsx
   â””â”€ Final Blackboard Entry:
       {
           entry_type: "excel_processing_result",
           source_agent: "excel_agent",
           source_task_id: "3",
           parent_task_id: "2",  â† Linked to OCR task
           content: { 
               output_file: "/output/analysis_report.xlsx",
               chain_trace: {
                   source_agent: "pdf_agent",
                   source_task: "1",
                   processing_task: "3"
               }
           },
           chain_next_agents: []  â† Chain ends
       }

6. Aggregation (No more chains)
   â””â”€ Return to aggregate_results
```

### Agent-Specific Chain Behavior

#### PDF Agent â†’ OCR Chain

```python
def _execute_pdf_task(state: AgentState) -> AgentState:
    # ... execute PDF operation ...
    
    # Check for extracted images
    extracted_images = result_data['findings'].get('extracted_images', [])
    if extracted_images:
        blackboard_entry['file_pointers'] = {
            "ocr_agent": ",".join(extracted_images)
        }
        blackboard_entry['chain_next_agents'] = ["ocr_agent"]
```

**Trigger Conditions**:
- `findings['images_found']` == True
- `findings['extracted_images']` has non-empty list
- File count > 0

#### OCR Agent â†’ Excel Chain

```python
def _execute_ocr_task(state: AgentState) -> AgentState:
    # ... extract from images ...
    
    # Check for table data
    extracted_table = result_data['findings'].get('extracted_table', [])
    if extracted_table:
        blackboard_entry['file_pointers'] = {
            "excel_agent": "table_data_extracted"
        }
        blackboard_entry['chain_next_agents'] = ["excel_agent"]
```

**Trigger Conditions**:
- `findings['extracted_table']` is non-empty
- Table structure validation passes
- Excel agent configuration enabled

#### WebSearch Agent â†’ Excel Chain

```python
def _execute_web_search_task(state: AgentState) -> AgentState:
    # ... perform web search ...
    
    # Check for CSV generation
    csv_file = result_data['output']['generated_files'].get('csv')
    if csv_file:
        blackboard_entry['file_pointers'] = {
            "excel_agent": csv_file
        }
        blackboard_entry['chain_next_agents'] = ["excel_agent"]
```

**Trigger Conditions**:
- `output['generated_files']['csv']` exists
- CSV file is valid and readable
- Excel processing is enabled

#### Excel Agent (Terminal Node)

Excel agent never chains further:
```python
# Excel always returns to aggregate_results
workflow.add_edge("execute_excel_task", "aggregate_results")
```

### Performance Characteristics

| Metric | Value | Notes |
|--------|-------|-------|
| **Chain Overhead** | < 50ms | File pointer lookup + condition check |
| **IO Reduction** | 30-50% | Skip file re-reads, use blackboard |
| **Latency Impact** | Neutral | No network calls added |
| **Memory Usage** | +5-10% | Blackboard entry storage |
| **Scalability** | Linear | O(n) where n = chain length |

### Benefits & Use Cases

#### Efficiency
- **Before**: PDF â†’ aggregate â†’ select â†’ OCR â†’ aggregate â†’ select â†’ Excel
- **After**: PDF â†’ (auto) OCR â†’ (auto) Excel â†’ aggregate
- **Savings**: 2-3 task selection cycles per chain

#### Data Integrity
- File pointers maintain exact paths
- No manual path construction
- Prevents file loss or corruption

#### Traceability
- Blackboard tracks full data lineage
- Each entry records source agent & task
- Chain trace shows complete processing path

#### Flexibility
- Agents decide chain conditions independently
- New agents can be added to chains easily
- Conditions can be customized per deployment

### Example Scenarios

**Scenario 1: PDF Report Analysis**
```
User: "Analyze the quarterly report"
      â†“
PDF Agent extracts text + finds 2 charts
      â†“ (chain)
OCR Agent analyzes charts, finds trend table
      â†“ (chain)
Excel Agent creates pivot table from trends
      â†“
Results: Structured analysis with visualizations
```

**Scenario 2: Web Research to Report**
```
User: "Research market trends and create report"
      â†“
WebSearch Agent scrapes data, generates CSV
      â†“ (chain)
Excel Agent processes CSV, creates summary stats
      â†“
Results: Report-ready spreadsheet
```

**Scenario 3: Complex Document Processing**
```
User: "Extract and analyze all forms from package"
      â†“
PDF Agent finds embedded forms (images)
      â†“ (chain)
OCR Agent extracts form data, finds tables
      â†“ (chain)
Excel Agent converts to structured database
      â†“
Results: Fully digitized, normalized data
```

### Specialized Sub-Agents

**Purpose**: Handle PDF document operations.

**Operations** (5 total):

| Operation | Input | Output | Use Case |
|-----------|-------|--------|----------|
| **read_pdf** | filepath, pages | text, metadata | Extract text from PDF |
| **create_pdf** | content, title, author | filepath | Generate new PDF |
| **merge_pdfs** | filepaths, output_path | merged_path | Combine multiple PDFs |
| **extract_pages** | filepath, page_nums, output | pages_path | Extract specific pages |
| **add_metadata** | filepath, metadata | updated_path | Add PDF metadata |

**Dependencies**: PyPDF2, pypdf, reportlab

### 2. Excel Agent (task_manager/sub_agents/excel_agent.py - 656 lines)

**Purpose**: Handle spreadsheet operations.

**Operations** (6 total):

| Operation | Input | Output | Use Case |
|-----------|-------|--------|----------|
| **read_excel** | filepath, sheet_name | {rows} | Read spreadsheet data |
| **create_excel** | rows, columns, filepath | created_path | Create new spreadsheet |
| **write_data** | filepath, sheet, cell, value | updated_path | Write to cells |
| **format_sheet** | filepath, sheet, format | formatted_path | Apply formatting |
| **append_data** | filepath, sheet, rows | appended_path | Add rows |
| **delete_sheet** | filepath, sheet_name | updated_path | Remove sheet |

**Dependencies**: openpyxl, pandas

### 3. OCR & Image Agent (task_manager/sub_agents/ocr_image_agent.py - 340+ lines)

**Purpose**: Handle OCR and image processing with optional multimodal vision analysis.

**Operations** (8 total):

| Operation | Input | Output | Use Case |
|-----------|-------|--------|----------|
| **ocr_image** | image_path, lang | {text, confidence} | Extract text from image |
| **extract_images_from_pdf** | pdf_path | [image_paths] | Get images from PDF |
| **batch_ocr** | [image_paths], lang | {results} | Process multiple images |
| **detect_language** | text | language_code | Identify text language |
| **extract_table_from_image** | image_path | {table_data} | Extract tabular data |
| **convert_image** | image_path, format | converted_path | Convert image format |
| **analyze_image** | image_path | {analysis} | Analyze image content |
| **analyze_visual_content** (NEW) | image_path, prompt | {vision_analysis} | LLM vision analysis for charts/diagrams |

**Dependencies**: pytesseract, EasyOCR, Pillow, OpenCV, langchain-google-genai (or openai/anthropic)

#### Multimodal Vision Analysis (NEW - Phase 4)

**Feature**: Leverage LLM vision capabilities for sophisticated visual content analysis beyond traditional OCR.

**Capabilities**:
- **Chart/Graph Analysis**: Describe trends, axes, data points, insights
- **Diagram Understanding**: Analyze flowcharts, organizational charts, technical diagrams
- **Heatmap Interpretation**: Understand color scales, spatial patterns
- **Table Recognition**: Extract structured data from complex table layouts
- **Form Processing**: Analyze form layouts and extract field relationships

**Configuration Options**:
```
ENABLE_VISION_ANALYSIS=true                    # Enable vision analysis
VISION_LLM_PROVIDER=google|openai|anthropic   # Vision LLM provider
VISION_LLM_MODEL=gemini-2.5-pro-vision        # Specific model (optional)
AUTO_DETECT_CHARTS=true                       # Auto-route complex images to vision
```

**Supported Vision Models**:
- Google Gemini: `gemini-2.5-pro-vision`, `gemini-pro-vision`
- OpenAI: `gpt-4o`, `gpt-4o-mini`
- Anthropic: `claude-3-5-sonnet-20241022`, `claude-3-5-haiku-20241022`

**Vision Analysis Methods**:
1. `analyze_visual_content(image_path, prompt, auto_detect)` - Main vision analysis operation
2. `_detect_visual_content_type(image_path)` - Auto-detect charts, diagrams, heatmaps
3. `_detect_lines()` - Detect chart axes and patterns
4. `_detect_color_gradients()` - Identify heatmaps and color scales
5. `_detect_structured_layout()` - Find tables and forms
6. `_detect_shapes()` - Recognize diagrams and flowcharts
7. `analyze_with_fallback()` - Vision with standard OCR fallback
8. `_initialize_vision_llm()` - Setup vision provider

**Workflow**:
```
Image Input
    â”‚
    â”œâ”€â†’ Auto-detect content type (if enabled)
    â”‚    â”œâ”€â†’ Charts/Graphs detected? â†’ Use vision analysis
    â”‚    â”œâ”€â†’ Heatmaps detected? â†’ Use vision analysis
    â”‚    â”œâ”€â†’ Tables detected? â†’ Use vision analysis
    â”‚    â”œâ”€â†’ Diagrams detected? â†’ Use vision analysis
    â”‚    â””â”€â†’ No complex content? â†’ Use standard OCR
    â”‚
    â””â”€â†’ Vision LLM Analysis
         â”œâ”€â†’ Generate context-aware prompt
         â”œâ”€â†’ Call multimodal LLM with image + prompt
         â””â”€â†’ Return structured analysis with insights
```

**Error Handling**:
- If vision analysis unavailable: Fall back to standard OCR
- If vision LLM call fails: Return OCR results with fallback indicator
- Graceful degradation ensures robustness

### 4. Web Search Agent (task_manager/sub_agents/web_search_agent.py - 800+ lines)

**Purpose**: Handle web search, static scraping, and dynamic JavaScript-heavy content extraction.

**Operations** (7 total):

| Operation | Input | Output | Use Case |
|-----------|-------|--------|----------|
| **search** | query, max_results | [{url, title, snippet}] | DuckDuckGo web search |
| **scrape** | url, selectors | {text, links, images} | Static HTML parsing with BeautifulSoup |
| **fetch** | url | {text, metadata, headers} | Download page content |
| **summarize** | url, max_sentences | {summary} | Extract and summarize text |
| **smart_scrape** | url, wait_selector, handle_pagination | {text, pages_scraped} | Dynamic JavaScript content via Playwright headless browser |
| **capture_screenshot** | url, selector | {screenshot_path} | Full-page or element screenshot for Vision agent analysis |
| **handle_pagination** | url, next_button_selector | {pages_content} | Follow pagination/Load More buttons, accumulate multi-page content |

**Dependencies**: duckduckgo-search, beautifulsoup4, requests, playwright (optional), langchain

**Static Scraping**:
- search: DuckDuckGo integration with configurable result count
- scrape: BeautifulSoup HTML parsing, extracts text/links/images with limits
- fetch: Raw content download with headers, optional file save
- summarize: Text extraction via scrape + simple sentence-based summarization

**Dynamic Content** (Playwright):
- **smart_scrape**: Headless Chromium browser, JavaScript rendering, networkidle wait, scrolling for lazy-loaded content, automatic Load More button detection (via common selectors), configurable page limits
- **capture_screenshot**: Full-page or element-specific screenshots (PNG), saved to temp directory, designed for Vision agent multimodal analysis of visual content
- **handle_pagination**: Smart next-button following, href extraction or click operations, content accumulation across pages

**Playwright Features**:
- Headless Chromium browser automation
- Wait until "networkidle" for dynamic content loading
- CSS selector-based waits before extraction
- Scroll-based lazy-loading support
- Automatic bot detection bypass
- Element visibility checks
- Timeout handling with fallback strategies

**Vision Agent Integration**:
- Screenshots posted to blackboard with path reference
- OCRImageAgent chains automatically for chart/diagram analysis
- Support for both full-page and element-specific captures

**Chain Execution**:
- Generates screenshots â†’ chains to OCR Agent for visual content analysis
- Scrapes CSV/table data â†’ chains to Excel Agent for processing
- Search results â†’ can feed into fetch/scrape/smart_scrape for deep analysis

### 5. Code Interpreter Agent (task_manager/sub_agents/code_interpreter_agent.py - 360 lines)

**Purpose**: Generate and execute Python code for data analysis and computational tasks.

**Operations** (4 total):

| Operation | Input | Output | Use Case |
|-----------|-------|--------|----------|
| **execute_analysis** | request, data_context | {output, code, files} | Execute natural language analysis |
| **generate_code** | request, data_context | {code} | Generate Python code from request |
| **execute_code** | code, data_context | {output, execution_time} | Run Python code in subprocess |
| **analyze_data** | request, data_context | {output, code, files} | Alias for execute_analysis |

**Dependencies**: pandas, numpy, matplotlib, langchain

**Capabilities**:
- Natural language to Python code generation via LLM
- Automatic code execution in isolated subprocess
- Chart/visualization generation (PNG, PDF, SVG)
- Data analysis with pandas/numpy/matplotlib
- Output capture and error handling
- Generated file tracking
- Blackboard integration for results

**Generated File Types**:
- Images: PNG, JPG, JPEG, GIF, BMP
- Charts: PDF, SVG
- Data: CSV, JSON, XLSX
- Other: Text logs, configuration

**Chain Execution**:
- Generates images/charts â†’ can chain to OCR Agent for analysis
- Generates CSV/data files â†’ can chain to Excel Agent for processing
- Full blackboard integration for cross-agent workflows

---

## Deep Hierarchies Enhancement

### Phase 1.5: Multi-Level Task Decomposition

This enhancement enables sophisticated deep hierarchical analysis with context preservation across levels.

### Hierarchy Levels

```
Level 0 (Root)
â”œâ”€â”€ task_id: "plan_0"
â”œâ”€â”€ parent_id: None
â”œâ”€â”€ depth: 0
â”œâ”€â”€ status: EXECUTING
â””â”€ children: [plan_1, plan_2, plan_3]

Level 1 (First Children)
â”œâ”€â”€ plan_1
â”‚   â”œâ”€â”€ parent_id: "plan_0"
â”‚   â”œâ”€â”€ depth: 1
â”‚   â”œâ”€â”€ context_summary: {parent findings}
â”‚   â””â”€ children: [plan_1_1, plan_1_2]
â”‚
â”œâ”€â”€ plan_2
â”‚   â”œâ”€â”€ parent_id: "plan_0"
â”‚   â”œâ”€â”€ depth: 1
â”‚   â””â”€ children: [plan_2_1]
â”‚
â””â”€â”€ plan_3
    â”œâ”€â”€ parent_id: "plan_0"
    â”œâ”€â”€ depth: 1
    â””â”€ children: []

Level 2 (Grandchildren)
â”œâ”€â”€ plan_1_1
â”‚   â”œâ”€â”€ parent_id: "plan_1"
â”‚   â”œâ”€â”€ depth: 2
â”‚   â””â”€ parent_context: {findings from plan_1}
â”‚
â””â”€â”€ plan_1_2
    â”œâ”€â”€ parent_id: "plan_1"
    â”œâ”€â”€ depth: 2
    â””â”€ parent_context: {findings from plan_1}

... continues up to depth_limit (default 5)
```

### Context Inheritance Flow

```
Execution at Depth 0
â”œâ”€â”€ Execute task
â”œâ”€â”€ Post findings:
â”‚   â””â”€â”€ {entry_type, content, source_agent, depth_level=0, parent_task_id=None}
â”œâ”€â”€ Blackboard Query:
â”‚   â”œâ”€â”€ query_by_depth(0) â†’ root findings
â”‚   â””â”€â”€ query_by_parent(None) â†’ root findings
â””â”€â”€ Before decomposition:
    â””â”€â”€ extract_context_from_parent("plan_0", blackboard)
        â”œâ”€â”€ Collects: All findings where parent_task_id=plan_0
        â”œâ”€â”€ Organizes: By entry_type
        â””â”€â”€ Returns: {parent_task_id, findings_by_type, total_count}

Execution at Depth 1 (plan_1)
â”œâ”€â”€ Inherit: parent_context from plan_0
â”œâ”€â”€ Execute task with parent insights
â”œâ”€â”€ Post nested findings:
â”‚   â””â”€â”€ {entry_type, content, source_agent, depth_level=1, parent_task_id="plan_0"}
â”œâ”€â”€ Blackboard Queries:
â”‚   â”œâ”€â”€ query_by_depth(1) â†’ all depth 1 findings
â”‚   â”œâ”€â”€ query_by_parent("plan_0") â†’ plan_0's children findings
â”‚   â””â”€â”€ query_by_parent("plan_1") â†’ plan_1's children findings
â””â”€â”€ Decompose plan_1 if depth < limit:
    â”œâ”€â”€ Check: current_depth(1) < depth_limit(5) âœ“
    â”œâ”€â”€ Extract: context_from_parent("plan_1")
    â””â”€â”€ Create: [plan_1_1, plan_1_2, plan_1_3]

Execution at Depth 2+ (plan_1_1)
â”œâ”€â”€ Inherit: parent_context from plan_1
â”œâ”€â”€ Execute with parent + grandparent insights
â”œâ”€â”€ Post double-nested findings:
â”‚   â””â”€â”€ {entry_type, content, source_agent, depth_level=2, parent_task_id="plan_1"}
â””â”€â”€ Continue recursively until depth_limit reached
```

### New MasterPlanner Methods for Deep Hierarchies

```python
# Posting nested findings
post_nested_finding(blackboard, entry_type, content, source_agent, 
                    source_task_id, parent_task_id, depth_level)

# Querying by parent task
query_blackboard_by_parent(blackboard, parent_task_id) -> [findings]

# Querying by depth level
query_blackboard_by_depth(blackboard, depth_level) -> [findings]

# Extracting context from parent
extract_context_from_parent(plan, parent_task_id, blackboard) -> dict

# Validating depth constraints
check_depth_limit(current_depth, depth_limit) -> bool

# Attaching context to tasks
add_context_to_plan_node(plan, task_id, context) -> [PlanNode]

# Establishing parent-child relationships
establish_parent_child_relationship(plan, parent_id, child_ids) -> [PlanNode]

# Analyzing task distribution
get_hierarchy_depth_distribution(plan) -> {depth: count}
```

### Depth Limit Protection

```python
# Default configuration
depth_limit = 5  # Maximum 5 levels deep

# Enforcement during execution
if current_depth >= depth_limit:
    logger.warning(f"Depth {current_depth} >= limit {depth_limit}")
    # Do not create subtasks
else:
    # Safe to create subtasks
    create_subtasks()
    current_depth += 1
```

---

## Configuration & Setup

**For detailed configuration instructions, see [README.md](README.md#configuration)**

Quick reference:
- **LLM Providers**: Anthropic Claude, OpenAI GPT, Google Gemini, Ollama
- **Configuration Methods**: Environment variables (.env), explicit config, dictionary/JSON
- **Key Settings**: provider, model, temperature, max_iterations, timeout, log_level

Initialization template:
```python
from task_manager import TaskManagerAgent, AgentConfig
from task_manager.config import EnvConfig

EnvConfig.load_env_file()
config = AgentConfig.from_env(prefix="AGENT_")
agent = TaskManagerAgent(objective="...", config=config)
```

### LLM Endpoint Configuration (v2.4+)

Generic LLM endpoint configuration allows TaskManager to work with any LLM provider without vendor lock-in:

**Environment Variables** (all optional):
```bash
LLM_API_KEY=your-api-key                    # Generic API key (or provider-specific alternatives)
LLM_API_BASE_URL=https://api.provider.com   # Custom API base URL
LLM_API_ENDPOINT_PATH=v1                    # Custom API endpoint path
LLM_API_VERSION=v1alpha                     # Custom API version
USE_NATIVE_SDK=false                        # Use native provider SDK if available
```

**Configuration Hierarchy** (in precedence order):
1. **Direct Parameters**: LLMClient(api_base_url="...")
2. **LLMConfig Fields**: LLMConfig(api_base_url="...")
3. **Environment Variables**: LLM_API_BASE_URL, LLM_API_ENDPOINT_PATH
4. **Provider Defaults**: Based on provider (Google, OpenAI, Anthropic, Ollama)

**Implementation Details**:
- Located in: `task_manager/utils/llm_client.py` (generic, multi-provider)
- Reads from generic `LLM_*` environment variables
- Supports any LLM provider (Google, OpenAI, Anthropic, Ollama, custom)
- Fallback to provider-specific variables for backward compatibility (e.g., GOOGLE_API_KEY)
- Constructs endpoint URL: `{base_url}/{endpoint_path}`
- Passes configuration through: Agent â†’ LLMConfig â†’ LLMClient
- Vision agents (`ocr_image_agent.py`) also support generic endpoint configuration
- Uses native provider SDKs when available/enabled, falls back to LangChain wrappers

**Example - Using Custom Endpoint**:
```python
# Via environment
LLM_API_BASE_URL=https://my-api.example.com
LLM_API_ENDPOINT_PATH=v1beta
USE_NATIVE_SDK=true

# Or via code
config = AgentConfig(
    llm=LLMConfig(
        provider="google",
        model_name="gemini-2.5-flash",
        api_base_url="https://my-api.example.com",
        api_endpoint_path="v1beta"
    )
)
```

---

## API Reference

### TaskManagerAgent

```python
class TaskManagerAgent:
    """Main orchestration engine"""
    
    def __init__(
        self,
        objective: str,
        metadata: Optional[dict] = None,
        config: Optional[AgentConfig] = None
    ) -> None:
        """Initialize agent with objective and config"""
    
    def execute(self) -> dict:
        """Execute the agent workflow"""
        # Returns: {
        #     "status": "completed" | "failed",
        #     "results": {...},
        #     "blackboard": [findings...],
        #     "history": [steps...],
        #     "plan": [tasks...]
        # }
```

### MasterPlanner

```python
class MasterPlanner:
    """Hierarchical planning & knowledge coordination"""
    
    @staticmethod
    def create_initial_plan(objective: str) -> List[PlanNode]:
        """Create initial hierarchical plan"""
    
    @staticmethod
    def get_next_ready_task(plan: List[PlanNode]) -> Optional[PlanNode]:
        """Get next task with status=READY"""
    
    @staticmethod
    def post_finding(
        blackboard: List[BlackboardEntry],
        entry: BlackboardEntry
    ) -> List[BlackboardEntry]:
        """Post new finding to blackboard"""
    
    @staticmethod
    def query_blackboard(
        blackboard: List[BlackboardEntry],
        criteria: dict
    ) -> List[BlackboardEntry]:
        """Query findings by criteria"""
    
    @staticmethod
    def query_blackboard_by_parent(
        blackboard: List[BlackboardEntry],
        parent_task_id: str
    ) -> List[BlackboardEntry]:
        """Get findings organized under parent task"""
    
    @staticmethod
    def query_blackboard_by_depth(
        blackboard: List[BlackboardEntry],
        depth_level: int
    ) -> List[BlackboardEntry]:
        """Get findings at specific depth"""
    
    @staticmethod
    def extract_context_from_parent(
        plan: List[PlanNode],
        parent_task_id: str,
        blackboard: List[BlackboardEntry]
    ) -> Dict[str, Any]:
        """Extract parent's findings for child context"""
    
    @staticmethod
    def check_depth_limit(
        current_depth: int,
        depth_limit: int
    ) -> bool:
        """Check if can decompose further"""
    
    @staticmethod
    def record_step(
        history: List[HistoryEntry],
        step_name: str,
        agent: str,
        task_id: str,
        outcome: dict,
        duration_seconds: float,
        error: Optional[str] = None
    ) -> List[HistoryEntry]:
        """Record execution step"""
```

### Sub-Agents Interface

```python
class PDFAgent:
    def execute_task(self, operation: str, parameters: dict) -> dict:
        """Execute PDF operation"""
        # operations: read_pdf, create_pdf, merge_pdfs, extract_pages, add_metadata

class ExcelAgent:
    def execute_task(self, operation: str, parameters: dict) -> dict:
        """Execute Excel operation"""
        # operations: read_excel, create_excel, write_data, format_sheet, append_data, delete_sheet

class OCRImageAgent:
    def execute_task(self, operation: str, parameters: dict) -> dict:
        """Execute OCR operation"""
        # operations: ocr_image, extract_images_from_pdf, batch_ocr, detect_language, 
        #            extract_table_from_image, convert_image, analyze_image

class WebSearchAgent:
    def execute_task(self, operation: str, parameters: dict) -> dict:
        """Execute web search operation"""
        # operations: search, scrape, fetch, summarize

class CodeInterpreterAgent:
    def execute_task(self, operation: str, parameters: dict) -> dict:
        """Execute code analysis operation"""
        # operations: execute_analysis, generate_code, execute_code, analyze_data
```

---

## Production Deployment

**See [README.md - Production Deployment](README.md#production-deployment) for:**
- Docker containerization
- Kubernetes deployment
- Environment setup
- Deployment checklist

**Performance Metrics**:
- Initialization: < 2 seconds
- Memory: < 500MB typical
- Scalable to 5+ hierarchy levels
- Type-safe, 100% type hints
- 0 compilation errors

---

## Vision Analysis Enhancement (v2.1)

### Overview

The system now includes **Multimodal Vision Analysis** capabilities that leverage LLM vision models to analyze complex visual content beyond traditional text-based OCR.

### Problem Solved

Traditional OCR excels at extracting text but struggles with:
- **Charts & Graphs**: Axis understanding, trend analysis, data point extraction
- **Diagrams**: Component identification, relationship understanding, process flows
- **Heatmaps**: Color scale interpretation, spatial pattern analysis
- **Complex Tables**: Layout relationships, nested structures, merged cells
- **Technical Drawings**: Symbol recognition, connection understanding

Vision-capable LLMs solve these by understanding visual semantics, not just pixel patterns.

### Architecture

**Vision Analysis Flow**:
```
Image Input
    â†“
[Auto-Detect Content Type]  â† New _detect_visual_content_type()
    â”œâ”€ Analyze visual characteristics
    â”œâ”€ Detect lines (axes), colors, structures, shapes
    â””â”€ Classify as: chart, heatmap, table, diagram, or text
    â†“
[Route Decision]
    â”œâ”€ Complex content detected? â†’ Use Vision LLM
    â””â”€ Simple text? â†’ Use Standard OCR
    â†“
[Vision LLM Analysis]  â† New VisionLLMWrapper
    â”œâ”€ Generate contextual prompt based on content type
    â”œâ”€ Encode image to base64
    â”œâ”€ Call multimodal LLM (Gemini/GPT-4o/Claude)
    â””â”€ Return structured analysis with insights
    â†“
[Fallback Handling]
    â”œâ”€ Vision unavailable? â†’ Fall back to OCR
    â”œâ”€ Vision failed? â†’ Return OCR results
    â””â”€ Graceful degradation ensures robustness
```

### Supported Vision Models

**Google Gemini**:
- `gemini-2.5-pro-vision` (latest, recommended)
- `gemini-pro-vision`

**OpenAI GPT**:
- `gpt-4o` (most capable)
- `gpt-4o-mini`

**Anthropic Claude**:
- `claude-3-5-sonnet-20241022` (vision-capable)
- `claude-3-5-haiku-20241022`

### Key Components

#### 1. Content Detection Methods
- `_detect_visual_content_type()` - Main detection orchestrator
- `_detect_lines()` - Identifies chart axes using edge detection
- `_detect_color_gradients()` - Identifies heatmaps and color scales
- `_detect_structured_layout()` - Finds tables and forms
- `_detect_shapes()` - Recognizes diagrams and shapes

#### 2. Vision Analysis Methods
- `analyze_visual_content()` - Primary vision analysis operation
- `_initialize_vision_llm()` - Sets up multimodal LLM provider
- `_prepare_image_for_llm()` - Encodes image to base64
- `_generate_analysis_prompt()` - Creates contextual prompts
- `analyze_with_fallback()` - Vision with OCR fallback
- `VisionLLMWrapper` - Unified provider interface

#### 3. VisionLLMWrapper Class
Provides unified interface across vision providers:
- Abstracts provider-specific message formats
- Handles image encoding (base64)
- Manages provider-specific parameters
- Returns normalized response format

### Configuration

**Environment Variables**:
```bash
# Enable/disable vision analysis
ENABLE_VISION_ANALYSIS=true                    # Default: true

# Vision LLM provider
VISION_LLM_PROVIDER=google|openai|anthropic   # Default: AGENT_LLM_PROVIDER

# Optional: specific vision model
VISION_LLM_MODEL=gemini-2.5-pro-vision        # Default: provider-specific

# Auto-detect content and route intelligently
AUTO_DETECT_CHARTS=true                       # Default: true
```

**Programmatic Configuration**:
```python
from task_manager.sub_agents.ocr_image_agent import OCRImageAgent

# Initialize with custom LLM
agent = OCRImageAgent(llm=my_vision_llm)

# Or use environment configuration
agent = OCRImageAgent()  # Auto-loads from .env
```

### Usage Examples

**Basic Vision Analysis**:
```python
result = agent.analyze_visual_content(
    image_path="chart.png",
    analysis_prompt="Describe the trends and key insights"
)
# Returns: {
#   "success": True,
#   "analysis": "...",
#   "content_detected": ["chart_or_graph"],
#   "provider": "google",
#   "metadata": {...}
# }
```

**Auto-Detect with Smart Routing**:
```python
result = agent.analyze_visual_content(
    image_path="document.png",
    auto_detect=True  # Detects content type automatically
)
# If chart detected: Uses vision analysis
# If simple text: Would use OCR
# Generates appropriate prompt based on type
```

**Fallback Pattern**:
```python
result = agent.analyze_with_fallback(
    image_path="complex_image.png",
    analysis_prompt="Extract data and trends"
)
# Tries vision analysis first
# Falls back to OCR if vision unavailable
# Returns: {"method": "ocr_fallback", ...}
```

**As Operation in TaskManagerAgent**:
```python
result = agent.execute_task(
    operation="analyze_visual_content",
    parameters={
        "image_path": "chart.png",
        "analysis_prompt": "What are the top 3 trends?",
        "auto_detect": True
    }
)
```

### Content Detection Details

**Chart/Graph Detection**:
- Detects axis lines using Hough transform
- Checks for line patterns
- Triggers prompt: "Analyze this chart/graph. Describe: 1) Type of chart, 2) Axes and units, 3) Key trends..."

**Heatmap Detection**:
- Analyzes color variance across image
- High variance indicates color gradients
- Triggers prompt: "Analyze this heatmap. Describe: 1) Variable represented, 2) Color scale, 3) Spatial patterns..."

**Table Detection**:
- Finds grid patterns
- Counts rectangular contours
- Triggers prompt: "Analyze this table. Extract: 1) Headers, 2) Data structure, 3) Key values..."

**Diagram Detection**:
- Detects circles and shapes
- Counts objects
- Triggers prompt: "Analyze this diagram. Describe: 1) Components, 2) Connections, 3) Process/hierarchy..."

### Error Handling

**Graceful Degradation**:
1. Vision analysis unavailable â†’ Use standard OCR
2. Vision LLM call fails â†’ Return OCR results with fallback indicator
3. Image cannot be loaded â†’ Return meaningful error message
4. Vision provider misconfigured â†’ Log warning, suggest action

**Result Indicators**:
```python
result = {
    "success": True,
    "method": "vision_analysis",     # or "ocr_fallback" or "standard_ocr"
    "analysis": "...",
    "vision_available": True,        # Whether vision was available
    "provider": "google",             # Which provider was used
}
```

### Performance Characteristics

**Detection Phase**:
- O(n) where n = image pixels
- Typically < 100ms for detection
- No API calls required

**Vision Analysis Phase**:
- API call to vision LLM (1-2 seconds typical)
- Varies by provider and model
- No local ML model needed
- Automatic retries with exponential backoff

**Fallback**:
- OCR fallback (if needed): 200-500ms
- No additional API calls
- Preserves robustness

### API Integration Points

**From TaskManagerAgent**:
```python
# Agent can request vision analysis during task execution
analysis = ocr_agent.analyze_visual_content(image_path, prompt)
```

**From Workflow**:
```python
# Execute_ocr_task node can use vision analysis
state = agent._execute_ocr_task(state)
# Automatically uses vision for complex images
```

**From Other Agents**:
```python
# Any agent can ask OCR agent for vision analysis
ocr_result = ocr_agent.execute_task(
    operation="analyze_visual_content",
    parameters={...}
)
```

### Configuration Best Practices

1. **API Keys**: Ensure vision LLM API key is set (often same as AGENT_LLM_PROVIDER)
2. **Model Selection**: Use latest vision model for best results
3. **Auto-Detect**: Leave enabled for intelligent routing
4. **Fallback**: Vision analysis has graceful OCR fallback
5. **Monitoring**: Check logs for vision analysis usage patterns

### Dependencies

**Python Libraries** (auto-detected):
- `numpy` - Image analysis
- `opencv-python` - Line and shape detection
- `pillow` - Image handling

**LLM Libraries** (choose provider):
- `langchain-google-genai` - Google Gemini
- `langchain-openai` - OpenAI GPT-4o
- `langchain-anthropic` - Anthropic Claude

### Future Enhancements

Potential improvements for future versions:
1. **Caching**: Cache vision analysis results for identical images
2. **Batch Processing**: Analyze multiple images in parallel
3. **Custom Models**: Support fine-tuned vision models
4. **Local Vision**: Integration with local vision models (ONNX)
5. **Structured Output**: Return JSON schema for programmatic access
6. **Multi-turn Analysis**: Follow-up questions on analyzed images

---

## Summary

**TaskManager** is a production-ready multi-agent orchestration system delivering:

âœ… **Recursive Task Decomposition** - Up to 5 levels with context preservation  
âœ… **Cross-Agent Workflows** - Automatic chaining (PDFâ†’OCRâ†’Excel) via file pointers  
âœ… **Knowledge Sharing** - Blackboard pattern for findings across agents  
âœ… **Research Synthesis** - Automatic contradiction detection with human escalation  
âœ… **Visual Understanding** - Multimodal LLM vision for charts, diagrams, heatmaps  
âœ… **Multi-Provider Support** - Anthropic, OpenAI, Google, Ollama  
âœ… **Complete Traceability** - Audit trails, execution history, findings lineage  
âœ… **Production Ready** - 100% type hints, 0 errors, comprehensive tests  

**Quick Architecture Navigation**:
- Core Orchestration: `task_manager/core/agent.py` (1330 lines)
- Hierarchical Planning: `task_manager/core/master_planner.py` (550+ lines)
- Workflow Routing: `task_manager/core/workflow.py` (142 lines)
- Data Models: `task_manager/models/state.py` (115 lines)
- Specialized Agents: `task_manager/sub_agents/` (PDF, Excel, OCR, WebSearch)

**Usage & Configuration**: See [README.md](README.md)

---

**TaskManager v2.4** | Production Ready âœ… | January 25, 2026

