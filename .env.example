# ============================================================================
# Task Manager Agent - Configuration Example
# ============================================================================
#
# Copy this file to .env and fill in your actual values.
# IMPORTANT: Never commit .env to version control!
# Add .env to .gitignore
#
# ============================================================================
# SELECT YOUR LLM PROVIDER AND FILL IN ONLY THOSE SETTINGS
# ============================================================================

# ============================================================================
# OPTION 1: ANTHROPIC CLAUDE (Recommended)
# ============================================================================

ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
AGENT_LLM_PROVIDER=anthropic
AGENT_LLM_MODEL=claude-sonnet-4-20250514


# ============================================================================
# OPTION 2: OPENAI GPT
# ============================================================================
# Uncomment to use OpenAI instead of Anthropic

# OPENAI_API_KEY=sk-your-openai-api-key-here
# AGENT_LLM_PROVIDER=openai
# AGENT_LLM_MODEL=gpt-4-turbo


# ============================================================================
# OPTION 3: GOOGLE GENERATIVE AI (GEMINI)
# ============================================================================
# Uncomment to use Google Generative AI instead

GOOGLE_API_KEY=AIzaSyBkjfkjNffRFZBFRbgt0F0ayjzduYSImMM
AGENT_LLM_PROVIDER=google
AGENT_LLM_MODEL=gemini-2.5-flash


# ============================================================================
# OPTION 4: LOCAL LLM (OLLAMA)
# ============================================================================
# Uncomment to use a local LLM via Ollama
# First, start Ollama: ollama run llama2

# AGENT_LLM_PROVIDER=local
# AGENT_LLM_MODEL=llama2
# AGENT_LLM_BASE_URL=http://localhost:11434


# ============================================================================
# AGENT SETTINGS (Optional - defaults are provided)
# ============================================================================

# Maximum number of iterations for the agent to run
AGENT_MAX_ITERATIONS=100

# Enable web search capability (true/false)
AGENT_ENABLE_SEARCH=true

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
AGENT_LOG_LEVEL=INFO

# Request timeout in seconds
AGENT_TIMEOUT=30

# Maximum number of retries for failed operations
AGENT_MAX_RETRIES=3

# Debug mode (true/false)
AGENT_DEBUG=false


# ============================================================================
# LOGGING & OBSERVABILITY SETTINGS (v2.5)
# ============================================================================

# Log folder path (where log files will be stored)
# Default: ./logs
AGENT_LOG_FOLDER=./logs

# Enable console logging (true/false)
AGENT_ENABLE_CONSOLE_LOGGING=true

# Enable file logging (true/false)
AGENT_ENABLE_FILE_LOGGING=true

# Log file rotation size (bytes) - default 10MB (10485760 bytes)
AGENT_LOG_MAX_BYTES=10485760

# Number of backup log files to keep
AGENT_LOG_BACKUP_COUNT=5

# ============================================================================
# LANGFUSE OBSERVABILITY (v2.5)
# ============================================================================
# Langfuse provides comprehensive observability for LLM applications
# Get started: https://langfuse.com/
# Installation: pip install langfuse

# Enable Langfuse integration (true/false)
ENABLE_LANGFUSE=false

# Langfuse Public Key (from https://cloud.langfuse.com/)
LANGFUSE_PUBLIC_KEY=pk_...

# Langfuse Secret Key (from https://cloud.langfuse.com/)
LANGFUSE_SECRET_KEY=sk_...

# Langfuse Host (optional - use for self-hosted or custom domain)
# Default: https://cloud.langfuse.com
# LANGFUSE_HOST=https://your-langfuse-instance.com


# LLM-specific settings

# Temperature (0.0-2.0) - Lower = more deterministic, Higher = more creative
AGENT_LLM_TEMPERATURE=0.2

# Maximum tokens to generate (optional)
AGENT_LLM_MAX_TOKENS=2000


# ============================================================================
# WEB SEARCH SETTINGS (Optional - for WebSearchAgent)
# ============================================================================

# Search engine/backend to use with DDGS (DuckDuckGo Search library)
# Options: 
#   - 'auto' (default): Automatically selects best available engine
#   - 'duckduckgo': Use DuckDuckGo search
#   - 'google': Use Google search  
#   - 'brave': Use Brave search
#   - 'yahoo': Use Yahoo search
#   - 'yandex': Use Yandex search
#   - 'mojeek': Use Mojeek search
#   - 'wikipedia': Use Wikipedia search
#   - 'grokipedia': Use Grokipedia search
#
# Note: DDGS queries multiple engines and aggregates results automatically.
# Setting a specific engine can be faster but may have fewer results.
WEBSEARCH_BACKEND=auto

# Search region/language code
# Examples: 
#   - 'wt-wt' (Worldwide, all languages)
#   - 'us' (United States)
#   - 'uk' (United Kingdom)
#   - 'de' (Germany)
#   - 'fr' (France)
#   - 'jp' (Japan)
# Full list: https://duckduckgo.com/params
WEBSEARCH_REGION=wt-wt

# Search timeout in seconds
WEBSEARCH_TIMEOUT=10

# Custom search URL (optional - reserved for future custom search engine integration)
# Leave empty to use DDGS default behavior
# WEBSEARCH_CUSTOM_URL=


# ============================================================================
# HOW TO USE THIS FILE
# ============================================================================
#
# 1. Copy this file to .env in your project root:
#    cp .env.example .env
#
# 2. Select one LLM provider and uncomment its settings
#
# 3. Replace the placeholder values with your actual API keys:
#    - Get Anthropic API key from: https://console.anthropic.com/
#    - Get OpenAI API key from: https://platform.openai.com/
#    - Get Google API key from: https://makersuite.google.com/
#    - For Ollama, install from: https://ollama.ai/
#
# 4. Adjust agent settings as needed for your use case
#
# 5. Never commit .env to version control:
#    - Make sure .env is in .gitignore
#    - Share .env.example instead for configuration reference
#
# ============================================================================
# SECURING YOUR API KEYS
# ============================================================================
#
# Best Practices:
# 1. Never hardcode API keys in your code
# 2. Use environment variables (.env file)
# 3. Keep .env files out of version control
# 4. Rotate API keys regularly
# 5. Use separate API keys for development and production
# 6. Monitor API usage for suspicious activity
# 7. Consider using API key management services for production
#
# ============================================================================
# INSTALLING REQUIRED DEPENDENCIES
# ============================================================================
#
# Base installation:
#   pip install langchain langchain-core langchain-community
#
# For Anthropic:
#   pip install langchain-anthropic
#
# For OpenAI:
#   pip install langchain-openai
#
# For Google:
#   pip install langchain-google-genai
#
# For Local LLM (Ollama):
#   pip install langchain-community
#   # Then install Ollama from https://ollama.ai/
#
# For .env file support:
#   pip install python-dotenv
#
# ============================================================================
# GETTING API KEYS
# ============================================================================
#
# Anthropic Claude:
#   1. Go to https://console.anthropic.com/
#   2. Create an account or log in
#   3. Navigate to API keys section
#   4. Click "Create Key"
#   5. Copy the key to ANTHROPIC_API_KEY
#
# OpenAI:
#   1. Go to https://platform.openai.com/
#   2. Create an account or log in
#   3. Navigate to API keys (https://platform.openai.com/api/keys)
#   4. Click "Create new secret key"
#   5. Copy the key to OPENAI_API_KEY
#
# Google Generative AI:
#   1. Go to https://makersuite.google.com/
#   2. Create an account or log in
#   3. Click "Get API key"
#   4. Copy the key to GOOGLE_API_KEY
#
# Local LLM (Ollama):
#   1. Install Ollama from https://ollama.ai/
#   2. Run: ollama pull llama2  (or any other model)
#   3. Start Ollama: ollama serve (runs on localhost:11434)
#   4. Set AGENT_LLM_BASE_URL=http://localhost:11434
#
# ============================================================================
